{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "n_gram_maxent.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNflrVNa0bWM14imguKTaQI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1190303311/AI/blob/main/n_gram_maxent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "Eo-JZTj9dHvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk, nltk.classify.util, nltk.metrics\n",
        "from nltk.classify import MaxentClassifier\n",
        "from nltk.collocations import BigramCollocationFinder\n",
        "from nltk.metrics import BigramAssocMeasures\n",
        "from nltk.probability import FreqDist, ConditionalFreqDist\n",
        "nltk.download('movie_reviews')\n",
        "\n",
        "from nltk.classify import MaxentClassifier\n",
        "from nltk.corpus import movie_reviews\n",
        "\n",
        "def word_feats(words):\n",
        " return dict([(word, True) for word in words])\n",
        "\n",
        "negids = movie_reviews.fileids('neg')\n",
        "posids = movie_reviews.fileids('pos')\n",
        "\n",
        "negfeats = [(word_feats(movie_reviews.words(fileids=[f])), 'neg') for f in negids]\n",
        "posfeats = [(word_feats(movie_reviews.words(fileids=[f])), 'pos') for f in posids]\n",
        "\n",
        "negcutoff = int(len(negfeats)*3/4)\n",
        "poscutoff = int(len(posfeats)*3/4)\n",
        "\n",
        "trainfeats = negfeats[:negcutoff] + posfeats[:poscutoff]\n",
        "classifier = nltk.MaxentClassifier.train(trainfeats)\n",
        "\n",
        "algorithm = nltk.classify.MaxentClassifier.ALGORITHMS[0]\n",
        "classifier = nltk.MaxentClassifier.train(train_features, algorithm,max_iter=10)\n",
        "\n",
        "classifier.show_most_informative_features(10)\n",
        "\n",
        "#all_words = nltk.FreqDist(word for word in movie_reviews.words())\n",
        "#top_words = set(all_words.keys()[:300])\n",
        "\n",
        "def word_feats(words):\n",
        "    return {word:True for word in words if word in top_words}"
      ],
      "metadata": {
        "id": "DBR-w_RhPzQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk, nltk.classify.util, nltk.metrics\n",
        "from nltk.classify import MaxentClassifier\n",
        "from nltk.collocations import BigramCollocationFinder\n",
        "from nltk.metrics import BigramAssocMeasures\n",
        "from nltk.probability import FreqDist, ConditionalFreqDist\n",
        "from nltk.classify import MaxentClassifier\n",
        "dataset = load_dataset(\"sst\", \"default\")\n",
        "train_features = dataset['train']\n",
        "test_features = dataset['test']\n",
        "feat_nltk = []\n",
        "for k,y in zip(train_features['tokens'],train_features['label']):\n",
        "  feat = {}\n",
        "  if y>0.5: target=1 \n",
        "  else: target=0\n",
        "  for i in k.split('|'):\n",
        "    feat[i] = True\n",
        "  feat_nltk.append((feat,target))\n",
        "algorithm = nltk.classify.MaxentClassifier.ALGORITHMS[0]\n",
        "classifier = nltk.MaxentClassifier.train(trainfeats, algorithm,max_iter=10)"
      ],
      "metadata": {
        "id": "ThNIowlCp1TG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainfeats[0]\n"
      ],
      "metadata": {
        "id": "pA8BIoNcajW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feats = []\n",
        "for i in trainfeats:\n",
        "  feat=[]\n",
        "  feat.append(i[1])\n",
        "  feat.extend(i[0].keys())\n",
        "  feats.append(feat)\n",
        "feats[0]"
      ],
      "metadata": {
        "id": "3fiEwUYlb5eq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import math\n",
        "from collections import defaultdict\n",
        "from datasets import load_dataset\n",
        "import operator\n",
        "\n",
        "class MaxEnt(object):\n",
        "  def __init__(self):\n",
        "    self.samples = []   #样本集合，（x,y)x是完整的一个样本\n",
        "    self.Y = set([])   #标签\n",
        "    self.numXY = defaultdict(int) #特征（x,y),count\n",
        "    self.N = 0     #样本数量\n",
        "    self.n = 0     #特征数量\n",
        "    self.xyID = {}    #特征索引\n",
        "    self.c = 0\n",
        "    self.ep = []     #样本分布特征期望\n",
        "    self.EP = []    #模型分布特征期望\n",
        "    self.w = []    #n个特征的权值\n",
        "    self.prew = []  #上一轮权值\n",
        "    self.eps = 0.001\n",
        "\n",
        "  def load_data_gram(self, features):\n",
        "    for i,feat in enumerate(features):\n",
        "      y = feat[0]\n",
        "      X = feat[1:]\n",
        "      self.samples.append(feat)\n",
        "      self.Y.add(y)\n",
        "      for x in X:\n",
        "        self.numXY[(x,y)] += 1\n",
        "    print(len(self.numXY))\n",
        "    self.numXY = {(a,b):self.numXY[(a,b)] for a,b in self.numXY if self.numXY[(a,b)]>10 and self.numXY[(a,b)]<300}\n",
        "    print(len(self.numXY))\n",
        "\n",
        "  def initparams(self):\n",
        "    self.N = len(self.samples)\n",
        "    self.n = len(self.numXY)\n",
        "    self.c = max([len(k)-1 for k in self.samples])\n",
        "    self.w = [0.0]*self.n\n",
        "    self.prew = self.w[:]\n",
        "    self.sample_ep();\n",
        "\n",
        "  def convergence(self):\n",
        "    for w, lw in zip(self.w, self.prew):\n",
        "      #print(math.fabs(w-lw))\n",
        "      if math.fabs(w-lw) >=self.eps:\n",
        "        return False\n",
        "    return True;\n",
        "\n",
        "  def sample_ep(self):\n",
        "    self.ep = [0.0]*self.n\n",
        "    for i, xy in enumerate(self.numXY):\n",
        "      self.ep[i] = self.numXY[xy]*1.0/self.N\n",
        "      self.xyID[xy] = i\n",
        "\n",
        "  def zx(self, X):\n",
        "    zx = 0.0\n",
        "    for y in self.Y:\n",
        "      sum = 0.0\n",
        "      for x in X:\n",
        "        if (x,y) in self.numXY:\n",
        "          sum+=self.w[self.xyID[(x,y)]]\n",
        "      zx += math.exp(sum)\n",
        "    return zx\n",
        "\n",
        "  def pyx(self, X):\n",
        "    zx = self.zx(X)\n",
        "    res = []\n",
        "    for y in self.Y:\n",
        "      sum = 0.0\n",
        "      for x in X:\n",
        "        if (x,y) in self.numXY:\n",
        "          sum+=self.w[self.xyID[(x,y)]]\n",
        "      yp = 1.0/ zx * math.exp(sum)\n",
        "      res.append((y, yp))\n",
        "    return res\n",
        "\n",
        "  def model_ep(self):\n",
        "    self.EP = [0.0]*self.n\n",
        "    for sample in self.samples:\n",
        "      X = sample[1:]\n",
        "      pyx = self.pyx(X)\n",
        "      for y, p in pyx:\n",
        "        for x in X:\n",
        "          if (x,y) in self.numXY:\n",
        "            self.EP[self.xyID[(x,y)]]+=p*1.0/self.N\n",
        "  \n",
        "  def train(self, maxiter=10000):\n",
        "    self.initparams()\n",
        "    for i in range(maxiter):\n",
        "      if i%50==0:\n",
        "        print(self.w)\n",
        "        print('iter: ',i,'...')\n",
        "      self.prew = self.w[:]\n",
        "      self.model_ep()\n",
        "      for i, w in enumerate(self.w):\n",
        "        self.w[i] += 1.0/self.c*math.log(self.ep[i]/self.EP[i])\n",
        "      if self.convergence():\n",
        "        break\n",
        "\n",
        "  def predict(self, input):\n",
        "    X = input.strip().split('\\t')\n",
        "    prob = self.pyx(X)\n",
        "    return prob\n",
        "\n",
        "maxent = MaxEnt()\n",
        "maxent.load_data_gram(feats)\n",
        "maxent.train()\n",
        "test_features = negfeats[negcutoff:] + posfeats[poscutoff:]\n"
      ],
      "metadata": {
        "id": "dZMaBxoBb8IQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = []\n",
        "for feat in feats:\n",
        "  res.append(maxent.pyx(feat[1:]))\n",
        "predict=[]\n",
        "for k in res:\n",
        "  k = sorted(k, key=lambda x:x[1])\n",
        "  target = k[-1][0]\n",
        "  predict.append(target)\n",
        "label=[]\n",
        "for k in feats:\n",
        "  label.append(k[0])\n",
        "count=0\n",
        "n = len(label)\n",
        "for i in range(n):\n",
        "  if label[i]==predict[i]:\n",
        "    count+=1\n",
        "print(count, n, count/n)"
      ],
      "metadata": {
        "id": "1mGcDfAvgqvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feats = []\n",
        "for i in test_features:\n",
        "  feat=[]\n",
        "  feat.append(i[1])\n",
        "  feat.extend(i[0].keys())\n",
        "  feats.append(feat)\n",
        "res = []\n",
        "for feat in feats:\n",
        "  res.append(maxent.pyx(feat[1:]))\n",
        "predict=[]\n",
        "for k in res:\n",
        "  k = sorted(k, key=lambda x:x[1])\n",
        "  target = k[-1][0]\n",
        "  predict.append(target)\n",
        "label=[]\n",
        "for k in feats:\n",
        "  label.append(k[0])\n",
        "count=0\n",
        "n = len(label)\n",
        "for i in range(n):\n",
        "  if label[i]==predict[i]:\n",
        "    count+=1\n",
        "print(count, n, count/n)"
      ],
      "metadata": {
        "id": "MtHFJcshpROm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}