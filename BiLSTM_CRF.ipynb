{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BiLSTM-CRF.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOwzlHICyHfnGjZ4FkVD5B3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1190303311/AI/blob/main/BiLSTM_CRF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8sQ6_N0TFXGK"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.autograd as autgrad\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "torch.manual_seed(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BiLSTM+CRF进行NER任务，模型参考pytorch教程。\n",
        "数据集为conll2003，一轮batch_size=1的训练结果，\n",
        "得到的weighted f1 score为91.2。\n",
        "另有BERT-BILSTM-CRF版本，支持batch训练"
      ],
      "metadata": {
        "id": "fwZvbRWWPVwD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def argmax(vec):\n",
        "  _, idx = torch.max(vec, 1)\n",
        "  return idx.item()\n",
        "\n",
        "def prepare_sequence(seq, to_idx):\n",
        "  idxs = [to_idx.get(w, len(to_idx)-1) for w in seq]\n",
        "  return torch.tensor(idxs, dtype=torch.long)\n",
        "\n",
        "\n",
        "def log_sum_exp(vec):\n",
        "  max_score = vec[0, argmax(vec)]\n",
        "  max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
        "  return max_score + torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))"
      ],
      "metadata": {
        "id": "6WBcKUrtFnXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "START_TAG = '<START>'\n",
        "STOP_TAG = '<STOP>'\n",
        "class BiLSTM_CRF(nn.Module):\n",
        "  def __init__(self, vocab_size, tag_to_idx, embedding_dim, hidden_dim):\n",
        "    super(BiLSTM_CRF, self).__init__()\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.vocab_size = vocab_size\n",
        "    self.tag_to_idx = tag_to_idx\n",
        "    self.tagset_size = len(tag_to_idx)\n",
        "\n",
        "    self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
        "    self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2, num_layers=1, bidirectional=True)\n",
        "\n",
        "    self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
        "    #转移矩阵，CRF层参数,(i,j)表示从j转移到i的分数\n",
        "    self.transitions = nn.Parameter(torch.randn(self.tagset_size, self.tagset_size))\n",
        "    #不可能转移到开始标志，不可能从结束标志转移\n",
        "    self.transitions.data[tag_to_idx[START_TAG], :] = -10000\n",
        "    self.transitions.data[:, tag_to_idx[STOP_TAG]] = -10000\n",
        "\n",
        "    self.hidden = self.init_hidden()\n",
        "\n",
        "  def init_hidden(self):\n",
        "    return (torch.randn(2, 1, self.hidden_dim // 2),\n",
        "         torch.randn(2, 1, self.hidden_dim // 2))\n",
        "    \n",
        "  def _forward_alg(self, feats):\n",
        "    init_alphas = torch.full((1, self.tagset_size), -10000.)\n",
        "    init_alphas[0][self.tag_to_idx[START_TAG]] = 0\n",
        "\n",
        "    forward_var = init_alphas\n",
        "\n",
        "    #feats是一个句子\n",
        "    for feat in feats:\n",
        "      alphas_t = []\n",
        "      #动态规划\n",
        "      for next_tag in range(self.tagset_size):\n",
        "        emit_score = feat[next_tag].view(1, -1).expand(1, self.tagset_size)\n",
        "\n",
        "        trans_score = self.transitions[next_tag].view(1, -1)\n",
        "\n",
        "        next_tag_var = forward_var + trans_score + emit_score\n",
        "\n",
        "        alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
        "      forward_var = torch.cat(alphas_t).view(1, -1)\n",
        "    terminal_var = forward_var + self.transitions[self.tag_to_idx[STOP_TAG]]\n",
        "    #输出所有路径分数的log_sum_e()和\n",
        "    alpha = log_sum_exp(terminal_var)\n",
        "    return alpha\n",
        "\n",
        "  def _get_lstm_features(self, sentence):\n",
        "    self.hidden = self.init_hidden()\n",
        "    embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n",
        "    lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
        "    lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
        "    lstm_feats = self.hidden2tag(lstm_out)\n",
        "    return lstm_feats\n",
        "\n",
        "  def _score_sentence(self, feats, tags):\n",
        "    #计算真实路径的分数，发射分数、转移分数相加\n",
        "    score = torch.zeros(1)\n",
        "    #给句子添加开始标志\n",
        "    tags = torch.cat([torch.tensor([self.tag_to_idx[START_TAG]], dtype=torch.long), tags])\n",
        "    for i, feat in enumerate(feats):\n",
        "      #tags[i]对应feats[i-1]\n",
        "      score += self.transitions[tags[i+1], tags[i]] + feat[tags[i+1]]\n",
        "    score += self.transitions[self.tag_to_idx[STOP_TAG], tags[-1]]\n",
        "    return score\n",
        "\n",
        "  '''维特比解码，找出分数最高的路径：\n",
        "    当前时刻为t，需要选择t时刻的tag，对每一个可选的tag，计算上一时刻\n",
        "    每一类tag转移到当前tag的分数，选取最大的，得到的集合必然包含最终\n",
        "    结果\n",
        "  '''\n",
        "  def _viterbi_decode(self, feats):\n",
        "    #回溯\n",
        "    backpointers = []\n",
        "\n",
        "    init_vvars = torch.full((1, self.tagset_size), -10000.)\n",
        "    #在第一个时刻，都会选择START_TAG最为最大路径的上一时刻tag\n",
        "    init_vvars[0][self.tag_to_idx[START_TAG]] = 0\n",
        "\n",
        "    forward_var = init_vvars\n",
        "    #feat是一个句子的一个词\n",
        "    for feat in feats:\n",
        "      bptrs_t = []\n",
        "      viterbivars_t = []\n",
        "\n",
        "      for next_tag in range(self.tagset_size):\n",
        "        #对当前时刻可选的每一个tag\n",
        "        next_tag_var = forward_var + self.transitions[next_tag]\n",
        "        best_tag_id = argmax(next_tag_var)\n",
        "        bptrs_t.append(best_tag_id)\n",
        "        viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
        "      \n",
        "      forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
        "      #保存路径\n",
        "      backpointers.append(bptrs_t)\n",
        "\n",
        "    terminal_var = forward_var + self.transitions[self.tag_to_idx[STOP_TAG]]\n",
        "    best_tag_id = argmax(terminal_var)\n",
        "    path_score = terminal_var[0][best_tag_id]\n",
        "\n",
        "    best_path = [best_tag_id]\n",
        "    for bptrs_t in reversed(backpointers):\n",
        "      best_tag_id = bptrs_t[best_tag_id]\n",
        "      best_path.append(best_tag_id)\n",
        "\n",
        "    start = best_path.pop()\n",
        "    assert start == self.tag_to_idx[START_TAG]\n",
        "    best_path.reverse()\n",
        "    return path_score, best_path\n",
        "\n",
        "  def neg_log_likelihood(self, sentence, tags):\n",
        "    #最大化 gold_score/all_score\n",
        "    #取-log，score算法是所有位置的发射分数和转移分数之和，再e\n",
        "    feats = self._get_lstm_features(sentence)\n",
        "    forward_score = self._forward_alg(feats)\n",
        "    gold_score = self._score_sentence(feats, tags)\n",
        "    return forward_score - gold_score\n",
        "\n",
        "  def forward(self, sentence):\n",
        "    lstm_feats = self._get_lstm_features(sentence)\n",
        "    score, tag_seq = self._viterbi_decode(lstm_feats)\n",
        "    return score, tag_seq"
      ],
      "metadata": {
        "id": "TSYJhtBGGw3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "from datasets import list_datasets, load_dataset\n",
        "from pprint import pprint\n",
        "datasets = list_datasets()\n",
        "print(\"Number of datasets in the Datasets library: \", len(datasets), \"\\n\\n\")\n",
        "datasets = load_dataset('conll2003')"
      ],
      "metadata": {
        "id": "8IL0gMrpPizR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = datasets['train']\n",
        "train_tokens = train_set['tokens'][:-1]\n",
        "train_tags = train_set['ner_tags'][:-1]\n",
        "val_set = datasets['validation']\n",
        "val_tokens = val_set['tokens'][:-1]\n",
        "val_tags = val_set['ner_tags'][:-1]"
      ],
      "metadata": {
        "id": "MH_bL-KUT0Vu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#训练\n",
        "#定义embedding_dim 和lstm的hidden_dim\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 100\n",
        "\n",
        "word2idx = {}\n",
        "for sentence in train_tokens:\n",
        "  for word in sentence:\n",
        "    if word not in word2idx:\n",
        "      word2idx[word] = len(word2idx)\n",
        "word2idx['UNK'] = len(word2idx)\n",
        "tag2idx = {'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8, START_TAG: 9, STOP_TAG: 10}\n",
        "model = BiLSTM_CRF(len(word2idx), tag2idx, EMBEDDING_DIM, HIDDEN_DIM)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
        "\n",
        "for epoch in range(1):\n",
        "  a = 0\n",
        "  losses = 0.\n",
        "  for sentence, tags in zip(train_tokens, train_tags):\n",
        "    model.zero_grad()\n",
        "    input = prepare_sequence(sentence, word2idx)\n",
        "    tags = torch.tensor(tags, dtype=torch.long)\n",
        "\n",
        "    loss = model.neg_log_likelihood(input, tags)\n",
        "\n",
        "    loss.backward()\n",
        "    losses += loss.item()\n",
        "    optimizer.step()\n",
        "    a+=1\n",
        "    if a % 1000 == 0:\n",
        "      print('epoch:', epoch, ' input:', a, ' loss:',losses/1000)\n",
        "      losses = 0.\n",
        "\n",
        "torch.save(model,'save.pt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QaS41a1WUSAC",
        "outputId": "c52c49bf-695c-4d93-9cd7-2bb17983358e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0  input: 1000  loss: 6.418446699500084\n",
            "epoch: 0  input: 2000  loss: 3.626508648812771\n",
            "epoch: 0  input: 3000  loss: 2.3487684562802316\n",
            "epoch: 0  input: 4000  loss: 1.8104148924350738\n",
            "epoch: 0  input: 5000  loss: 1.8326457664966582\n",
            "epoch: 0  input: 6000  loss: 1.5888553948402404\n",
            "epoch: 0  input: 7000  loss: 1.8805180814862252\n",
            "epoch: 0  input: 8000  loss: 2.448946821987629\n",
            "epoch: 0  input: 9000  loss: 1.9829042331874371\n",
            "epoch: 0  input: 10000  loss: 2.5726412694454193\n",
            "epoch: 0  input: 11000  loss: 2.0982760944366454\n",
            "epoch: 0  input: 12000  loss: 2.5124796831011773\n",
            "epoch: 0  input: 13000  loss: 1.983550221800804\n",
            "epoch: 0  input: 14000  loss: 2.0402784041762354\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "total_tokens = 0\n",
        "right = 0\n",
        "pred = []\n",
        "y_true = []\n",
        "for sentence, tags in zip(val_tokens, val_tags):\n",
        "  input = prepare_sequence(sentence, word2idx)\n",
        "  y_true.extend(tags)\n",
        "  tags = torch.Tensor(tags)\n",
        "  score, tags_pred = model(input)\n",
        "  pred.extend(tags_pred)\n",
        "  total_tokens += len(input)\n",
        "  tags_pred = torch.Tensor(tags_pred)\n",
        "  res = tags == tags_pred\n",
        "  right += torch.sum(res)\n",
        "\n",
        "acurracy = right/total_tokens\n",
        "print(right, total_tokens, acurracy)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03zQSS1a1aWf",
        "outputId": "921dfe6c-ce14-4813-b6f4-7e789bf5bf13"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(46742) 51362 tensor(0.9101)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "a = f1_score(y_true, pred, average='weighted')\n",
        "a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6RyAj1GNvbr",
        "outputId": "94d95c2d-ec5c-42a8-8859-0013dd81280a"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9118695407465229"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    }
  ]
}