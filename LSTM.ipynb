{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPVUHIznrmt0oEZ9O4fH/WA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1190303311/AI/blob/main/LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LSTM模型单层实现，参考博客https://blog.csdn.net/FlyingLittlePig/article/details/72229041\n",
        "\n",
        "LSTM前向和反向传播：\n",
        "http://arunmallya.github.io/writeups/nn/lstm/index.html#/"
      ],
      "metadata": {
        "id": "9JxxkhaPgPOz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import nltk, itertools, csv\n",
        "import codecs\n",
        "\n",
        "TXTCODING = 'utf-8'\n",
        "unknown_token = 'UNKNOWN_TOKEN'\n",
        "start_token = 'START_TOKEN'\n",
        "end_token = 'END_TOKEN'\n",
        "\n",
        "nltk.download('punkt')\n",
        "# 解析评论文件为数值向量\n",
        "class tokenFile2vector:\n",
        "    def __init__(self, file_path, dict_size):\n",
        "        self.file_path = file_path\n",
        "        self.dict_size = dict_size\n",
        "\n",
        "    # 将文本拆成句子，并加上句子开始和结束标志\n",
        "    def _get_sentences(self):\n",
        "        sents = []\n",
        "        with open(self.file_path, 'rb') as f:\n",
        "            #reader = csv.reader(f, skipinitialspace=True)\n",
        "            reader = csv.reader(codecs.iterdecode(f, 'utf-8'), skipinitialspace=True)\n",
        "            # 去掉表头 \n",
        "            # 解析每个评论为句子\n",
        "            next(reader)\n",
        "            sents = itertools.chain(*[nltk.sent_tokenize(x[0].lower()) for x in reader])\n",
        "            sents = ['%s %s %s' % (start_token, sent, end_token) for sent in sents]\n",
        "            print ('Get sentences.', len(sents))\n",
        "\n",
        "            return sents\n",
        "\n",
        "    # 得到每句话的单词，并得到字典及字典中每个词的下标\n",
        "    def _get_dict_wordsIndex(self, sents):\n",
        "        sent_words = [nltk.word_tokenize(sent) for sent in sents]\n",
        "        word_freq = nltk.FreqDist(itertools.chain(*sent_words))\n",
        "        print ('Get words.', len(word_freq))\n",
        "\n",
        "        common_words = word_freq.most_common(self.dict_size-1)\n",
        "        # 生成词典\n",
        "        dict_words = [word[0] for word in common_words]\n",
        "        dict_words.append(unknown_token)\n",
        "        # 得到每个词的下标，用于生成词向量\n",
        "        index_of_words = dict((word, ix) for ix, word in enumerate(dict_words))\n",
        "\n",
        "        return sent_words, dict_words, index_of_words\n",
        "\n",
        "    # 得到训练数据\n",
        "    def get_vector(self):\n",
        "        sents = self._get_sentences()\n",
        "        sent_words, dict_words, index_of_words = self._get_dict_wordsIndex(sents)\n",
        "\n",
        "        # 将每个句子中没包含进词典dict_words中的词替换为unknown_token\n",
        "        for i, words in enumerate(sent_words):\n",
        "            sent_words[i] = [w if w in dict_words else unknown_token for w in words]\n",
        "\n",
        "        X_train = np.array([[index_of_words[w] for w in sent[:-1]] for sent in sent_words])\n",
        "        y_train = np.array([[index_of_words[w] for w in sent[1:]] for sent in sent_words])\n",
        "\n",
        "        return X_train, y_train, dict_words, index_of_words\\\n",
        "\n",
        "file_path = 'results-20170508-103637.csv'\n",
        "dict_size = 8000\n",
        "myTokenFile = tokenFile2vector(file_path, dict_size)\n",
        "X_train, y_train, dict_words, index_of_words = myTokenFile.get_vector()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MdKKyHINOfKj",
        "outputId": "53c7dd22-a100-4cba-ca19-908aacda243e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "Get sentences. 24700\n",
            "Get words. 30347\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:56: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:57: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8xPEfF4P6qz",
        "outputId": "a7581dc0-087d-4195-8ef5-6f616343af57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[401, 1]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "处理好的文本输入格式example(X_train[0], y_train[0])：\n",
        "x: [1，2，3] y:[2,3,4]"
      ],
      "metadata": {
        "id": "t5gMWT7wgo9V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GwZ6aI1fSzVJ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def softmax(x):\n",
        "  x = np.array(x)\n",
        "  max_x = np.max(x)\n",
        "  return np.exp(x-max_x) / np.sum(np.exp(x-max_x))\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1.0/(1.0 + np.exp(-x))\n",
        "\n",
        "def tanh(x):\n",
        "  return (np.exp(x) - np.exp(-x))/(np.exp(x) + np.exp(-x))\n",
        "\n",
        "class myLSTM():\n",
        "  def __init__(self, data_dim, hidden_dim=100):\n",
        "    #data_dim就是输入维度（vocab_size)，hidden_dim是记忆细胞和隐藏层的维度（二者一样）\n",
        "    self.data_dim = data_dim\n",
        "    self.hidden_dim = hidden_dim\n",
        "\n",
        "    #初始化权值向量\n",
        "    self.whi, self.wxi, self.bi = self._init_wh_wx()\n",
        "    self.whf, self.wxf, self.bf = self._init_wh_wx()                           \n",
        "    self.who, self.wxo, self.bo = self._init_wh_wx()\n",
        "    self.wha, self.wxa, self.ba = self._init_wh_wx()\n",
        "    self.wy, self.by = np.random.uniform(-np.sqrt(1.0/self.hidden_dim), np.sqrt(1.0/self.hidden_dim), \n",
        "                                   (self.data_dim, self.hidden_dim)), \\\n",
        "                           np.random.uniform(-np.sqrt(1.0/self.hidden_dim), np.sqrt(1.0/self.hidden_dim), \n",
        "                                   (self.data_dim, 1))\n",
        "  def _init_wh_wx(self):\n",
        "    wh = np.random.uniform(-np.sqrt(1.0/self.hidden_dim), np.sqrt(1.0/self.hidden_dim), \n",
        "                                   (self.hidden_dim, self.hidden_dim))\n",
        "    wx = np.random.uniform(-np.sqrt(1.0/self.data_dim), np.sqrt(1.0/self.data_dim), \n",
        "                                   (self.hidden_dim, self.data_dim))\n",
        "    b = np.random.uniform(-np.sqrt(1.0/self.data_dim), np.sqrt(1.0/self.data_dim), \n",
        "                                   (self.hidden_dim, 1))\n",
        "\n",
        "    return wh, wx, b\n",
        "\n",
        "  def _init_s(self, T):\n",
        "    iss = np.array([np.zeros((self.hidden_dim, 1))] * (T + 1))  # input gate\n",
        "    fss = np.array([np.zeros((self.hidden_dim, 1))] * (T + 1))  # forget gate\n",
        "    oss = np.array([np.zeros((self.hidden_dim, 1))] * (T + 1))  # output gate\n",
        "    ass = np.array([np.zeros((self.hidden_dim, 1))] * (T + 1))  # current inputstate\n",
        "    hss = np.array([np.zeros((self.hidden_dim, 1))] * (T + 1))  # hidden state\n",
        "    css = np.array([np.zeros((self.hidden_dim, 1))] * (T + 1))  # cell state\n",
        "    ys = np.array([np.zeros((self.data_dim, 1))] * T)    # output value\n",
        "\n",
        "    return {'iss': iss, 'fss': fss, 'oss': oss, \n",
        "                'ass': ass, 'hss': hss, 'css': css, \n",
        "                'ys': ys}\n",
        "  def forward(self, x):\n",
        "    # 向量时间长度\n",
        "    T = len(x)        \n",
        "    # 初始化各个状态向量\n",
        "    stats = self._init_s(T)               \n",
        "\n",
        "    for t in range(T):\n",
        "            # 前一时刻隐藏状态\n",
        "      ht_pre = np.array(stats['hss'][t-1]).reshape(-1, 1)\n",
        "\n",
        "            # input gate\n",
        "      stats['iss'][t] = self._cal_gate(self.whi, self.wxi, self.bi, ht_pre, x[t], sigmoid)\n",
        "            # forget gate\n",
        "      stats['fss'][t] = self._cal_gate(self.whf, self.wxf, self.bf, ht_pre, x[t], sigmoid)\n",
        "            # output gate\n",
        "      stats['oss'][t] = self._cal_gate(self.who, self.wxo, self.bo, ht_pre, x[t], sigmoid)\n",
        "            # current inputstate\n",
        "      stats['ass'][t] = self._cal_gate(self.wha, self.wxa, self.ba, ht_pre, x[t], tanh)\n",
        "\n",
        "            # cell state, ct = ft * ct_pre + it * at\n",
        "      stats['css'][t] = stats['fss'][t] * stats['css'][t-1] + stats['iss'][t] * stats['ass'][t]            \n",
        "            # hidden state, ht = ot * tanh(ct)\n",
        "      stats['hss'][t] = stats['oss'][t] * tanh(stats['css'][t])\n",
        "\n",
        "            # output value, yt = softmax(self.wy.dot(ht) + self.by)\n",
        "      stats['ys'][t] = softmax(self.wy.dot(stats['hss'][t]) + self.by)\n",
        "\n",
        "    return stats\n",
        "  \n",
        "  def _cal_gate(self, wh, wx, b, ht_pre, x, activation):\n",
        "        return activation(wh.dot(ht_pre) + wx[:, x].reshape(-1,1) + b)\n",
        "\n",
        "\n",
        "  def predict(self, x):\n",
        "    stats = self.forward(x)\n",
        "    pre_y = np.argmax(stats['ys'].reshape(len(x), -1), axis=1)         \n",
        "    return pre_y\n",
        "\n",
        "  #softmax交叉熵损失，(x,y)多个样本\n",
        "  def loss(self, x, y):\n",
        "    cost = 0        \n",
        "    for i in range(len(y)):\n",
        "        stats = self.forward(x[i])\n",
        "            # 取出 y[i] 中每一时刻对应的预测值\n",
        "        pre_yi = stats['ys'][range(len(y[i])), y[i]]\n",
        "        cost -= np.sum(np.log(pre_yi))\n",
        "\n",
        "        # 统计所有y中词的个数, 计算平均损失\n",
        "    N = np.sum([len(yi) for yi in y])\n",
        "    ave_loss = cost / N\n",
        "\n",
        "    return ave_loss\n",
        "\n",
        "\n",
        "  def _init_wh_wx_grad(self):\n",
        "    dwh = np.zeros(self.whi.shape)\n",
        "    dwx = np.zeros(self.wxi.shape)\n",
        "    db = np.zeros(self.bi.shape)\n",
        "\n",
        "    return dwh, dwx, db\n",
        "\n",
        "\n",
        "  #x,y 为一个样本\n",
        "  def bptt(self, x, y):\n",
        "    dwhi, dwxi, dbi = self._init_wh_wx_grad()\n",
        "    dwhf, dwxf, dbf = self._init_wh_wx_grad()                           \n",
        "    dwho, dwxo, dbo = self._init_wh_wx_grad()\n",
        "    dwha, dwxa, dba = self._init_wh_wx_grad()\n",
        "    dwy, dby = np.zeros(self.wy.shape), np.zeros(self.by.shape)\n",
        "\n",
        "\n",
        "    delta_ct = np.zeros((self.hidden_dim, 1))\n",
        "    delta_ct_pre = np.zeros((self.hidden_dim, 1))\n",
        "\n",
        "    stats = self.forward(x)\n",
        "    #目标函数对y的偏导\n",
        "    delta_o = stats['ys']\n",
        "    delta_o[np.arange(len(y)), y] -= 1 #(o-t)，MSE loss\n",
        "\n",
        "    for t in np.arange(len(y))[::-1]:#倒序\n",
        "      #输出层wy、by的偏导\n",
        "      dwy += delta_o[t].dot(stats['hss'][t].reshape(1, -1))  \n",
        "      dby += delta_o[t]\n",
        "\n",
        "      # 目标函数对隐藏状态的偏导数\n",
        "      if t == len(y)-1:\n",
        "        delta_ht_pre = np.zeros((self.hidden_dim, 1))\n",
        "      else:\n",
        "        delta_C = stats['oss'][t+1] * (1-tanh(stats['css'][t+1])**2)\n",
        "        delta_c = self.whf.dot(stats['fss'][t+1] * (1-stats['fss'][t+1]) * stats['css'][t]) + \\\n",
        "              self.whi.dot(stats['iss'][t+1] * (1-stats['iss'][t+1]) * stats['ass'][t+1]) + \\\n",
        "              self.wha.dot(stats['iss'][t+1] * (1-stats['ass'][t+1]**2))\n",
        "        delta_hh = self.who.dot(stats['oss'][t+1] * (1-stats['oss'][t+1]) * tanh(stats['css'][t+1])) + delta_C * delta_c\n",
        "        delta_ht_pre = delta_ht_pre * delta_hh\n",
        "\n",
        "      delta_ht = self.wy.T.dot(delta_o[t]) + delta_ht_pre\n",
        "\n",
        "      # 各个门及状态单元的偏导数\n",
        "      delta_ot = delta_ht * tanh(stats['css'][t])\n",
        "      delta_ct += delta_ht * stats['oss'][t] * (1-tanh(stats['css'][t])**2) + delta_ct_pre\n",
        "      delta_it = delta_ct * stats['ass'][t]\n",
        "      delta_ft = delta_ct * stats['css'][t-1]\n",
        "      delta_at = delta_ct * stats['iss'][t]\n",
        "      \n",
        "      delta_at_net = delta_at * (1-stats['ass'][t]**2)\n",
        "      delta_it_net = delta_it * stats['iss'][t] * (1-stats['iss'][t])\n",
        "      delta_ft_net = delta_ft * stats['fss'][t] * (1-stats['fss'][t])\n",
        "      delta_ot_net = delta_ot * stats['oss'][t] * (1-stats['oss'][t])\n",
        "\n",
        "      dwhf, dwxf, dbf = self._cal_grad_delta(dwhf, dwxf, dbf, delta_ft_net, stats['hss'][t-1], x[t])                              \n",
        "      dwhi, dwxi, dbi = self._cal_grad_delta(dwhi, dwxi, dbi, delta_it_net, stats['hss'][t-1], x[t])                              \n",
        "      dwha, dwxa, dba = self._cal_grad_delta(dwha, dwxa, dba, delta_at_net, stats['hss'][t-1], x[t])            \n",
        "      dwho, dwxo, dbo = self._cal_grad_delta(dwho, dwxo, dbo, delta_ot_net, stats['hss'][t-1], x[t])\n",
        "\n",
        "      delta_ct_pre = delta_ct * stats['fss'][t]\n",
        "      delta_ht_pre = delta_ht\n",
        "\n",
        "    return [dwhf, dwxf, dbf,\n",
        "         dwhi, dwxi, dbi,\n",
        "         dwha, dwxa, dba,\n",
        "        dwho, dwxo, dbo,\n",
        "           dwy, dby]\n",
        "\n",
        "  def _cal_grad_delta(self, dwh, dwx, db, delta_net, ht_pre, x):\n",
        "    dwh += delta_net * ht_pre\n",
        "    dwx += delta_net * x\n",
        "    db += delta_net\n",
        "\n",
        "    return dwh, dwx, db\n",
        "\n",
        "  #计算梯度，x,y一个样本\n",
        "  def sgd_step(self, x, y, lr):\n",
        "    dwhf, dwxf, dbf, \\\n",
        "    dwhi, dwxi, dbi, \\\n",
        "    dwha, dwxa, dba, \\\n",
        "    dwho, dwxo, dbo, \\\n",
        "    dwy, dby  = self.bptt(x, y)\n",
        "\n",
        "    self.whf, self.wxf, self.bf = self._update_wh_wx(lr, self.whf, self.wxf, self.bf, dwhf, dwxf, dbf)\n",
        "    self.whi, self.wxi, self.bi = self._update_wh_wx(lr, self.whi, self.wxi, self.bi, dwhi, dwxi, dbi)\n",
        "    self.wha, self.wxa, self.ba = self._update_wh_wx(lr, self.wha, self.wxa, self.ba, dwha, dwxa, dba)\n",
        "    self.who, self.wxo, self.bo = self._update_wh_wx(lr, self.who, self.wxo, self.bo, dwho, dwxo, dbo)\n",
        "\n",
        "    self.wy, self.by = self.wy - lr*dwy, self.by-lr*dby\n",
        "\n",
        "  def _update_wh_wx(self, lr, wh, wx, b, dwh, dwx, db):\n",
        "    wh -= lr*dwh\n",
        "    wx -= lr*dwx\n",
        "    b -= lr*db\n",
        "\n",
        "    return wh, wx, b\n",
        "\n",
        "  def train(self, X_train, y_train, lr=0.005, n_epoch=5):\n",
        "    losses = []\n",
        "    num_examples = 0\n",
        "\n",
        "    for epoch in range(n_epoch):\n",
        "      for i in range(len(y_train)):\n",
        "        self.sgd_step(X_train[i], y_train[i], lr)\n",
        "        num_examples += 1\n",
        "      \n",
        "      loss = self.loss(X_train, y_train)\n",
        "      losses.append(loss)\n",
        "      print('epoch: ',epoch+1, 'loss = ', loss)\n",
        "      if len(losses) > 1 and losses[-1] > losses[-2]:\n",
        "        lr *= 0.5\n",
        "        print('decrease lr to ', lr)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lstm = myLSTM(8000, hidden_dim=100)\n",
        "lstm.train(X_train[:200], y_train[:200], lr=0.005, n_epoch=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9ff_MtDOeBG",
        "outputId": "1ecc9bb2-d432-4560-e25a-c97aafd7eb64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: RuntimeWarning: overflow encountered in exp\n",
            "  if __name__ == '__main__':\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:  1 loss =  6.430563203617257\n",
            "epoch:  2 loss =  6.128558763228318\n",
            "epoch:  3 loss =  6.005181943263411\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Aac8PL5mb5gr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}